---
title: "exploratoryAnalysis of the News Data set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r ,echo = TRUE, message=FALSE, warning=FALSE,results=FALSE}
library(tm) # Framework for text mining.
library(qdap) # Quantitative discourse analysis of transcripts.
library(dplyr) # Data wrangling, pipe operator %>%().
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(magrittr)
library(stringr)
library(wordcloud)
library(RWeka)
library(mlbench)
library(ngram)
library(dtplyr)
library(data.table)
```

## characterisating of The news text input files
    These files are extremely large.

### Input  news data set
```{r}
con <- file("en_us.news.txt")
newsInput <- readLines(con, skipNul = TRUE)
close(con)
# remove non-ascii characters
newsInput<- gsub("[^\x20-\x7E]", "", newsInput)
# remove all puncutation but single quote, period, question mark and exclaimation point
newsInput<- gsub("[^'.?![:^punct:]]", "", newsInput, perl=TRUE)
# remove numbers
newsInput <-gsub('[0-9]+', '', newsInput)
# remove quotation 
newsInput<- gsub("[\"]", "", newsInput)

newsDf <- data.frame(newsInput)
newsDf <-as.data.frame( newsDf[complete.cases(newsDf),])
```
### 2 thru 8 N-grams from  news 
```{r}

b <- 8
a <- 1
begin <- 1
end <- 7700
while(a <= b){
    newsDfR <- as.data.frame(newsDf[begin:end,]) # Create dataframe
    CorpusC = VCorpus(VectorSource(newsDfR)) # create news corpus from dataframe 

    CorpusC <- tm_map(CorpusC, tolower) # change all characters to lower case
    CorpusC <- tm_map(CorpusC, stripWhitespace)# strip white spaces
#strwrap(CorpusC[1])
    CorpusC<- tm_map(CorpusC, PlainTextDocument) # Convert to plain text document
    
## N-gram 2 thru 8 Calculation

### 2grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram2 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
# Write dataframe to csv file
outcon <- file(paste("WF_2gram", a, ".csv", sep=""))
write.csv(WF_Ngram2, file = outcon)

### 3grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram3 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_3gram", a, ".csv", sep=""))
write.csv(WF_Ngram3, file = outcon)

### 4grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram4 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_4gram", a, ".csv", sep=""))
write.csv(WF_Ngram4, file = outcon)

### 5grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram5 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_5gram", a, ".csv", sep=""))
write.csv(WF_Ngram5, file = outcon)

### 6grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram6 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_6gram", a, ".csv", sep=""))
write.csv(WF_Ngram6, file = outcon)

### 7grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram7 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_7gram", a, ".csv", sep=""))
write.csv(WF_Ngram7, file = outcon)

### 8grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram8 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("WF_8gram", a, ".csv", sep=""))
write.csv(WF_Ngram8, file = outcon)

#increment counters
a <- a + 1
temp <- end
begin <- end + 1
end <- end + 7700
}

```

______________________________________________________________________________________________________________________________
## Create unified NGram dataframe and csv files for 2 thru 8 N-grams
### Create unified 2-Gram dataframe and csv file

```{r}
# Create unified 2 NGram dataframe and csv files
Ngram2.1 <-as.data.frame(read.csv("WF_2gram1.csv"))
Ngram2.2 <-as.data.frame(read.csv("WF_2gram2.csv"))
Ngram2.3 <-as.data.frame(read.csv("WF_2gram3.csv"))
Ngram2.4 <-as.data.frame(read.csv("WF_2gram4.csv"))
Ngram2.5 <-as.data.frame(read.csv("WF_2gram5.csv"))
Ngram2.6 <-as.data.frame(read.csv("WF_2gram6.csv"))
Ngram2.7 <-as.data.frame(read.csv("WF_2gram7.csv"))
Ngram2.8 <-as.data.frame(read.csv("WF_2gram8.csv"))

Ngram2 <- rbind(Ngram2.1,Ngram2.2,Ngram2.3,Ngram2.4,Ngram2.5,Ngram2.6,Ngram2.7,Ngram2.8)
Ngram2 <- Ngram2 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram2$word <- gsub('[0-9]+', '', Ngram2$word)
ngramOrder <- order(Ngram2$word, decreasing=FALSE)
Ngram2 <- Ngram2[ngramOrder,]
#________________________________________________________________________
Ngram_2 <- data.frame()
begin <- 1
end <- 10000
z <- 1
numReps <- as.integer(NROW(Ngram2)/10000)

while(z <= numReps) { 
    NgramName <- Ngram2[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_2 <- rbind(Ngram_2,NgramName)
    z <- z + 1
}

# Write the 2 Ngram with the indificual words split and added to additional columns
write.csv(Ngram_2, file = "Ngram_2.csv")

```

### Create unified 3 NGram dataframe and csv files
```{r}
Ngram3.1 <-as.data.frame(read.csv("WF_3gram1.csv"))
Ngram3.2 <-as.data.frame(read.csv("WF_3gram2.csv"))
Ngram3.3 <-as.data.frame(read.csv("WF_3gram3.csv"))
Ngram3.4 <-as.data.frame(read.csv("WF_3gram4.csv"))
Ngram3.5 <-as.data.frame(read.csv("WF_3gram5.csv"))
Ngram3.6 <-as.data.frame(read.csv("WF_3gram6.csv"))
Ngram3.7 <-as.data.frame(read.csv("WF_3gram7.csv"))
Ngram3.8 <-as.data.frame(read.csv("WF_3gram8.csv"))

Ngram3 <- rbind(Ngram3.1,Ngram3.2,Ngram3.3,Ngram3.4,Ngram3.5,Ngram3.6,Ngram3.7,Ngram3.8)

Ngram3 <- Ngram3 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram3$word <- gsub('[0-9]+', '', Ngram3$word)
ngramOrder <- order(Ngram3$word, decreasing=FALSE)
Ngram3 <- Ngram3[ngramOrder,]

Ngram_3 <- data.frame()
numReps <- as.integer(NROW(Ngram3)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram3[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all 3-Ngram with expanded columns
    Ngram_3 <- rbind(Ngram_3,NgramName)
    z <- z + 1
}

write.csv(Ngram_3, file = "Ngram_3.csv")
```

### Create  4-NGram dataframe and csv files

```{r}
Ngram4.1 <-as.data.frame(read.csv("WF_4gram1.csv"))
Ngram4.2 <-as.data.frame(read.csv("WF_4gram2.csv"))
Ngram4.3 <-as.data.frame(read.csv("WF_4gram3.csv"))
Ngram4.4 <-as.data.frame(read.csv("WF_4gram4.csv"))
Ngram4.5 <-as.data.frame(read.csv("WF_4gram5.csv"))
Ngram4.6 <-as.data.frame(read.csv("WF_4gram6.csv"))
Ngram4.7 <-as.data.frame(read.csv("WF_4gram7.csv"))
Ngram4.8 <-as.data.frame(read.csv("WF_4gram8.csv"))

Ngram4 <- rbind(Ngram4.1,Ngram4.2,Ngram4.3,Ngram4.4,Ngram4.5,Ngram4.6,Ngram4.7,Ngram4.8)
Ngram4 <- Ngram4 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram4$word <- gsub('[0-9]+', '', Ngram4$word)
ngramOrder <- order(Ngram4$word, decreasing=FALSE)
Ngram4 <- Ngram4[ngramOrder,]
# write.csv(Ngram4, file = "Ngram4.csv")
Ngram_4 <- data.frame()
numReps <- as.integer(NROW(Ngram4)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram4[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_4 <- rbind(Ngram_4,NgramName)
    z <- z + 1
}
write.csv(Ngram_4, file = "Ngram_4.csv")

```
### Create 5-Ngram with the indivual words split and added to additional columns
```{r}

# Create unified 5 NGram dataframe and csv files
Ngram5.1 <-as.data.frame(read.csv("WF_5gram1.csv"))
Ngram5.2 <-as.data.frame(read.csv("WF_5gram2.csv"))
Ngram5.3 <-as.data.frame(read.csv("WF_5gram3.csv"))
Ngram5.4 <-as.data.frame(read.csv("WF_5gram4.csv"))
Ngram5.5 <-as.data.frame(read.csv("WF_5gram5.csv"))
Ngram5.6 <-as.data.frame(read.csv("WF_5gram6.csv"))
Ngram5.7 <-as.data.frame(read.csv("WF_5gram7.csv"))
Ngram5.8 <-as.data.frame(read.csv("WF_5gram8.csv"))

Ngram5 <- rbind(Ngram5.1,Ngram5.2,Ngram5.3,Ngram5.4,Ngram5.5,Ngram5.6,Ngram5.7,Ngram5.8)

Ngram5 <- Ngram5 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram5$word <- gsub('[0-9]+', '', Ngram5$word)
ngramOrder <- order(Ngram5$word, decreasing=FALSE)
Ngram5 <- Ngram5[ngramOrder,]
write.csv(Ngram5, file = "Ngram5.csv")

# Create dataframe column for each word in the string
Ngram_5 <- data.frame()
numReps <- as.integer(NROW(Ngram5)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram5[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_5 <- rbind(Ngram_5,NgramName)
    z <- z + 1
}
write.csv(Ngram_5, file = "Ngram_5.csv")
```

### Create unified 6 NGram dataframe and csv files
```{r}
Ngram6.1 <-as.data.frame(read.csv("WF_6gram1.csv"))
Ngram6.2 <-as.data.frame(read.csv("WF_6gram2.csv"))
Ngram6.3 <-as.data.frame(read.csv("WF_6gram3.csv"))
Ngram6.4 <-as.data.frame(read.csv("WF_6gram4.csv"))
Ngram6.5 <-as.data.frame(read.csv("WF_6gram5.csv"))
Ngram6.6 <-as.data.frame(read.csv("WF_6gram6.csv"))
Ngram6.7 <-as.data.frame(read.csv("WF_6gram7.csv"))
Ngram6.8 <-as.data.frame(read.csv("WF_6gram8.csv"))

Ngram6 <- rbind(Ngram6.1,Ngram6.2,Ngram6.3,Ngram6.4,Ngram6.5,Ngram6.6,Ngram6.7,Ngram6.8)

Ngram6 <- Ngram6 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram6$word <- gsub('[0-9]+', '', Ngram6$word)
ngramOrder <- order(Ngram6$word, decreasing=FALSE)
Ngram6 <- Ngram6[ngramOrder,]
write.csv(Ngram6, file = "Ngram6.csv")

# Create dataframe column for each word in the string
Ngram_6 <- data.frame()
numReps <- as.integer(NROW(Ngram6)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram6[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_6 <- rbind(Ngram_6,NgramName)
    z <- z + 1
}
write.csv(Ngram_6, file = "Ngram_6.csv")
```
```{r}
# Create unified 7 NGram dataframe and csv files
Ngram7.1 <-as.data.frame(read.csv("WF_7gram1.csv"))
Ngram7.2 <-as.data.frame(read.csv("WF_7gram2.csv"))
Ngram7.3 <-as.data.frame(read.csv("WF_7gram3.csv"))
Ngram7.4 <-as.data.frame(read.csv("WF_7gram4.csv"))
Ngram7.5 <-as.data.frame(read.csv("WF_7gram5.csv"))
Ngram7.6 <-as.data.frame(read.csv("WF_7gram6.csv"))
Ngram7.7 <-as.data.frame(read.csv("WF_7gram7.csv"))
Ngram7.8 <-as.data.frame(read.csv("WF_7gram8.csv"))

Ngram7 <- rbind(Ngram7.1,Ngram7.2,Ngram7.3,Ngram7.4,Ngram7.5,Ngram7.6,Ngram7.7,Ngram7.8)

Ngram7 <- Ngram7 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram7$word <- gsub('[0-9]+', '', Ngram7$word)
ngramOrder <- order(Ngram7$word, decreasing=FALSE)
Ngram7 <- Ngram7[ngramOrder,]

write.csv(Ngram7, file = "Ngram7.csv")

# Create dataframe column for each word in the string
Ngram_7 <- data.frame()
numReps <- as.integer(NROW(Ngram7)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram7[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_7 <- rbind(Ngram_7,NgramName)
    z <- z + 1
}
write.csv(Ngram_7, file = "Ngram_7.csv")
```
### Create unified 8 NGram dataframe and csv files
```{r}
Ngram8.1 <-as.data.frame(read.csv("WF_8gram1.csv"))
Ngram8.2 <-as.data.frame(read.csv("WF_8gram2.csv"))
Ngram8.3 <-as.data.frame(read.csv("WF_8gram3.csv"))
Ngram8.4 <-as.data.frame(read.csv("WF_8gram4.csv"))
Ngram8.5 <-as.data.frame(read.csv("WF_8gram5.csv"))
Ngram8.6 <-as.data.frame(read.csv("WF_8gram6.csv"))
Ngram8.7 <-as.data.frame(read.csv("WF_8gram7.csv"))
Ngram8.8 <-as.data.frame(read.csv("WF_8gram8.csv"))

Ngram8 <- rbind(Ngram8.1,Ngram8.2,Ngram8.3,Ngram8.4,Ngram8.5,Ngram8.6,Ngram8.7,Ngram8.8)

Ngram8 <- Ngram8 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram8$word <- gsub('[0-9]+', '', Ngram8$word)
ngramOrder <- order(Ngram8$word, decreasing=FALSE)
Ngram8 <- Ngram8[ngramOrder,]


write.csv(Ngram8, file = "Ngram8.csv")
```
```{r}
ngram_7 <-  read.csv("WF_7gram.csv")
ngram_7b <- as.data.frame( read.csv("WF_7gram2.csv"))
ngram7 <- rbind(ngram_7,ngram_7b)
ngram7$word <- gsub('[0-9]+', '', ngram7$word)
ngramOrder <- order(ngram7$word, decreasing=FALSE)
ngram7O <- ngram7[ngramOrder,]
```

### Create testing dataframe for news dataset
```{r}
con <- file("en_us.news.txt")
newsInput <- readLines(con, skipNul = TRUE)
close(con)
# remove non-ascii characters
newsInput<- gsub("[^\x20-\x7E]", "", newsInput)
# remove all puncutation but single quote, period, question mark and exclaimation point
newsInput<- gsub("[^'.?![:^punct:]]", "", newsInput, perl=TRUE)
# remove numbers
newsInput <-gsub('[0-9]+', '', newsInput)
# remove quotation 
newsInput<- gsub("[\"]", "", newsInput)



newsDf <- data.frame(newsInput)
newsDfR <-as.data.frame( newsDf[complete.cases(newsDf),])
begin <- 69301
end <- NROW(newsDf)
testingDf <- as.data.frame(newsDf[begin:end,]) #testing dataFrame


testCorpus = VCorpus(VectorSource(testingDf)) # create news corpus from dataframe 
testCorpus <- tm_map(testCorpus, tolower)# change all characters to lower case
testCorpus <- tm_map(testCorpus, stripWhitespace)# strip white spaces
testCorpus<- tm_map(testCorpus, PlainTextDocument)

### 4grams identification for news testing dataset
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm <- TermDocumentMatrix(testCorpus, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
newsTestNgram4 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("newsTest.csv", sep=""))
write.csv(newsTestNgram4, file = outcon)
```


### Developing Markov model for prediction of words with bigram.
```{r}
Ngram_2 <-as.data.frame(read.csv("Ngram_2.csv"))

# input string to predict next word
predictString <- "conduct adequate due diligence"

# Select the first two words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 

# Find the third word
nextWord <- Ngram_2[,4] == newVec[3]

vector_2 <- Ngram_2[nextWord,]
# remove meaning less words
vector_2 <- filter(vector_2, V4 != "a" & V4 != "and" & sum != 1) 
sortOrder <- order(vector_2$sum, decreasing=TRUE)
vectorOrder <- vector_2[sortOrder,]
#vector_4c <- Ngram_4[Ngram_4Secondc,]

fourthWorda <- as.character(vector_4a$V5)
fourthWordb <- as.character(vector_4b$V6)
#fourthWordc <- as.character(vector_4b$V6)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]



```
### Developing Markov model for prediction of words with trigram.
```{r}
Ngram_3 <-as.data.frame(read.csv("Ngram_3.csv"))

# input string to predict next word
predictString <- "you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.


Ngram_3Seconda <- Ngram_3[,4] == newVec[2] &  Ngram_3[,5] == newVec[3]
#Ngram_3Secondb <- Ngram_3[,4] == "to" &  Ngram_3[,5] == "city"


vector_1a <- Ngram_3[Ngram_3Seconda,]
#vector_1b <- Ngram_3[Ngram_3Second,]

```

### Developing Markov model for prediction of words with 4-gram.
```{r}
Ngram_4 <-as.data.frame(read.csv("Ngram_4.csv"))

# input string to predict next word
predictString <- "you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 
pick3 <- paste(newVec[1], newVec[2], newVec[3], sep=" ")

# match last two words of the string and find potential next words.
Ngram_4Seconda <- Ngram_4[,4] == newVec[2] &  Ngram_4[,5] == newVec[3]
Ngram_4Secondb <- Ngram_4[,5] == newVec[2] &  Ngram_4[,6] == newVec[3]
#Ngram_4Secondc <- Ngram_4[,6] == newVec[2] &  Ngram_4[,6] == newVec[3]


vector_4a <- Ngram_4[Ngram_4Seconda,]
vector_4b <- Ngram_4[Ngram_4Secondb,]
#vector_4c <- Ngram_4[Ngram_4Secondc,]

fourthWorda <- as.character(vector_4a$V5)
fourthWordb <- as.character(vector_4b$V6)
#fourthWordc <- as.character(vector_4b$V6)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]



```

### Developing Markov model for prediction of words with 5-gram.
```{r}
Ngram_5 <-as.data.frame(read.csv("Ngram_5.csv"))

# input string to predict next word
predictString <- "attend to city business"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.


# match last two words of the string and find potential next words.

Ngram_5Seconda <- Ngram_5[,3] == newVec[2] &  Ngram_5[,4] == newVec[3]
Ngram_5Secondb <- Ngram_5[,4] == newVec[2] &  Ngram_5[,5] == newVec[3]
Ngram_5Secondc <- Ngram_5[,5] == newVec[2] &  Ngram_5[,6] == newVec[3]
Ngram_5Secondd <- Ngram_5[,6] == newVec[2] &  Ngram_5[,7] == newVec[3]


vector_5a <- Ngram_5[Ngram_5Seconda,]
vector_5b <- Ngram_5[Ngram_5Secondb,]
vector_5c <- Ngram_5[Ngram_5Secondc,]
vector_5d <- Ngram_5[Ngram_5Secondd,]

fourthWorda <- as.character(vector_5a$V4)
fourthWordb <- as.character(vector_5b$V5)
fourthWordc <- as.character(vector_5b$V6)
fourthWordd <- as.character(vector_5b$V7) 

# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb, fourthWordc, fourthWordd)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]

```

### Developing Markov model for prediction of words with 6-gram.
```{r}
Ngram_6 <-as.data.frame(read.csv("Ngram_6.csv"))

# input string to predict next word
predictString <- "then you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.

Ngram_6Seconda <- Ngram_6[,3] == newVec[2] &  Ngram_6[,4] == newVec[3]
Ngram_6Secondb <- Ngram_6[,4] == newVec[2] &  Ngram_6[,5] == newVec[3]
Ngram_6Secondc <- Ngram_6[,5] == newVec[2] &  Ngram_6[,6] == newVec[3]
Ngram_6Secondd <- Ngram_6[,6] == newVec[2] &  Ngram_6[,7] == newVec[3]
Ngram_6Seconde <- Ngram_6[,7] == newVec[2] &  Ngram_6[,8] == newVec[3]


vector_6a <- Ngram_6[Ngram_6Seconda,]
vector_6b <- Ngram_6[Ngram_6Secondb,]
vector_6c <- Ngram_6[Ngram_6Secondc,]
vector_6d <- Ngram_6[Ngram_6Secondd,]
vector_6e <- Ngram_6[Ngram_6Seconde,]
fourthWorda <- as.character(vector_6a$V4)
fourthWordb <- as.character(vector_6b$V5)
fourthWordc <- as.character(vector_6b$V6)
fourthWordd <- as.character(vector_6b$V7)
fourthWorde <- as.character(vector_6b$V8)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb, fourthWordc, fourthWordd, fourthWorde)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]
```


