---
title: "exploratoryAnalysis of the News Data set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r ,echo = TRUE, message=FALSE, warning=FALSE,results=FALSE}
library(tm) # Framework for text mining.
library(qdap) # Quantitative discourse analysis of transcripts.
library(dplyr) # Data wrangling, pipe operator %>%().
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(magrittr)
library(stringr)
library(wordcloud)
library(RWeka)
library(mlbench)
library(ngram)
library(dtplyr)
library(data.table)
```

## characterisating of The news text input files
    These files are extremely large.

### Input  news data set
```{r}
con <- file("en_us.twitter.txt")
TwitterInput <- readLines(con, skipNul = TRUE)
close(con)
# remove non-ascii characters
TwitterInput<- gsub("[^\x20-\x7E]", "", TwitterInput)
# remove all puncutation but single quote, period, question mark and exclaimation point
TwitterInput<- gsub("[^'.?![:^punct:]]", "", TwitterInput, perl=TRUE)
# remove numbers
TwitterInput <-gsub('[0-9]+', '', TwitterInput)
# remove quotation 
TwitterInput<- gsub("[\"]", "", TwitterInput)

twitterDf <- data.frame(TwitterInput)
twitterDf <-as.data.frame( twitterDf[complete.cases(twitterDf),])
```
## Create new data frame from Training twitter
```{r}
    twitterTrainDfR <- as.data.frame(twitterDf[2356200:2360148,]) # Create dataframe
    CorpusC = VCorpus(VectorSource(twitterTrainDfR)) # create news corpus from dataframe 

    CorpusC <- tm_map(CorpusC, tolower) # change all characters to lower case
    CorpusC <- tm_map(CorpusC, stripWhitespace)# strip white spaces
    CorpusC<- tm_map(CorpusC, PlainTextDocument) # Convert to plain text document
    
    ### 4grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram4T <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

outcon <- file(paste("testDfT", ".csv", sep=""))
write.csv(WF_Ngram4T, file = outcon)
```


## Create unified NGram dataframe and csv files for 2 thru 4 N-grams
```{r}
Ngram2T <- data.frame()
Ngram3T <- data.frame()
Ngram4T <- data.frame()

a <- 1
ronNum <- as.integer(NROW(twitterDf)/7700)
begin <- 1
end <- 7700
while(a <= ronNum){
    newsDfR <- as.data.frame(twitterDf[begin:end,]) # Create dataframe
    CorpusC = VCorpus(VectorSource(newsDfR)) # create news corpus from dataframe 

    CorpusC <- tm_map(CorpusC, tolower) # change all characters to lower case
    CorpusC <- tm_map(CorpusC, stripWhitespace)# strip white spaces
#strwrap(CorpusC[1])
    CorpusC<- tm_map(CorpusC, PlainTextDocument) # Convert to plain text document
    
## N-gram Calculation

### 2grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram2 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))


### 3grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram3 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

### 4grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram4 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

### 5grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram5 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

# combine Ngrams into one dataframe
Ngram2T <- rbind(Ngram2T, WF_Ngram2)
Ngram3T <- rbind(Ngram3T, WF_Ngram3)
Ngram4T <- rbind(Ngram4T, WF_Ngram4)

# increment counters
a <- a + 1
temp <- end
begin <- end + 1
end <- end + 7700
}

# write Ngram2b to csv file
outcon <- file(paste("WF_2gramT", ".csv", sep=""))
write.csv(Ngram2T, file = outcon)
# write Ngram3b to csv file
outcon <- file(paste("WF_3gramT", ".csv", sep=""))
write.csv(Ngram3T, file = outcon)
# write Ngram4b to csv file
outcon <- file(paste("WF_4gramT", ".csv", sep=""))
write.csv(Ngram4T, file = outcon)


```
## Create unified NGram dataframe and csv files for 5 thru 6 N-grams
```{r}

Ngram5T <- data.frame()
Ngram6T <- data.frame()


a <- 1
ronNum <- as.integer(NROW(twitterDf)/7700)
begin <- 1
end <- 7700
while(a <= ronNum){
    twitterDfR <- as.data.frame(twitterDf[begin:end,]) # Create dataframe
    CorpusC = VCorpus(VectorSource(twitterDfR)) # create news corpus from dataframe 

    CorpusC <- tm_map(CorpusC, tolower) # change all characters to lower case
    CorpusC <- tm_map(CorpusC, stripWhitespace)# strip white spaces
#strwrap(CorpusC[1])
    CorpusC<- tm_map(CorpusC, PlainTextDocument) # Convert to plain text document
    
## N-gram Calculation


### 5grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram5 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

### 6grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram6 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

# combine Ngrams into one dataframe
Ngram5T <- rbind(Ngram5T, WF_Ngram5)
Ngram6T <- rbind(Ngram6T, WF_Ngram6)


# increment counters
a <- a + 1
temp <- end
begin <- end + 1
end <- end + 7700
}

# write Ngram5b to csv file
outcon <- file(paste("WF_5gramT", ".csv", sep=""))
write.csv(Ngram5T, file = outcon)
# write Ngram6b to csv file
outcon <- file(paste("WF_6gramT", ".csv", sep=""))
write.csv(Ngram6T, file = outcon)

```
## Create unified NGram dataframe and csv files for 7 thru 8 N-grams
```{r}
Ngram7T <- data.frame()
Ngram8T <- data.frame()

a <- 1
ronNum <- as.integer(NROW(twitterDf)/7700)
begin <- 1
end <- 7700
while(a <= ronNum){
    twitterDfR <- as.data.frame(twitterDf[begin:end,]) # Create dataframe
    CorpusC = VCorpus(VectorSource(twitterDfR)) # create news corpus from dataframe 

    CorpusC <- tm_map(CorpusC, tolower) # change all characters to lower case
    CorpusC <- tm_map(CorpusC, stripWhitespace)# strip white spaces
    CorpusC<- tm_map(CorpusC, PlainTextDocument) # Convert to plain text document
    
## N-gram Calculation

### 7grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram7 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))

### 8grams identification
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
tdm <- TermDocumentMatrix(CorpusC, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
WF_Ngram8 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))


# combine Ngrams into one dataframe
Ngram7T <- rbind(Ngram7T, WF_Ngram7)
Ngram8T <- rbind(Ngram8T, WF_Ngram8)

# increment counters
a <- a + 1
temp <- end
begin <- end + 1
end <- end + 7700
}

# write Ngram7b to csv file
outcon <- file(paste("WF_7gramT", ".csv", sep=""))
write.csv(Ngram7T, file = outcon)
# write Ngram8b to csv file
outcon <- file(paste("WF_8gramT",".csv", sep=""))
write.csv(Ngram8T, file = outcon)
```




___________________________________________________________________________________________________________________
## Create unified NGram dataframe and csv files for 2 thru 8 N-grams
### Create unified 2-Gram dataframe and csv file

```{r}
# Create unified 2 NGram dataframe and csv files
Ngram2T <-as.data.frame(read.csv("WF_2gramT.csv"))

Ngram2T <- Ngram2T %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram2T$word <- gsub('[0-9]+', '', Ngram2T$word)
ngramOrder <- order(Ngram2T$word, decreasing=FALSE)
Ngram2T <- Ngram2T[ngramOrder,]
#________________________________________________________________________
Ngram_2T <- data.frame()
begin <- 1
end <- 10000
z <- 1
NgramNumber <- 2
numReps <- as.integer(NROW(Ngram2T)/10000)
while(z <= numReps) { 
    NgramName <- Ngram2T[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_2T <- rbind(Ngram_2T,NgramName)
    z <- z + 1
}

# Write the 2 Ngram with the indificual words split and added to additional columns
write.csv(Ngram_2T, file = "Ngram_2T.csv")

```

### Create unified 3 NGram dataframe and csv files
```{r}
Ngram3T <-as.data.frame(read.csv("WF_3gramT.csv"))

Ngram3T <- Ngram3T %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram3T$word <- gsub('[0-9]+', '', Ngram3T$word)
ngramOrder <- order(Ngram3T$word, decreasing=FALSE)
Ngram3T <- Ngram3T[ngramOrder,]

Ngram_3T <- data.frame()
numReps <- as.integer(NROW(Ngram_3T)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram3T[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all 3-Ngram with expanded columns
    Ngram_3T <- rbind(Ngram_3T,NgramName)
    z <- z + 1
}

write.csv(Ngram_3T, file = "Ngram_3T.csv")
```

### Create  4-NGram dataframe and csv files

```{r}
Ngram4T <-as.data.frame(read.csv("WF_4gramT.csv"))

Ngram4T <- Ngram4T %>% group_by(word) %>% summarise( freq=sum(sum))

Ngram4T$word <- gsub('[0-9]+', '', Ngram4T$word)
ngramOrder <- order(Ngram4T$word, decreasing=FALSE)
Ngram4T <- Ngram4T[ngramOrder,]
# write.csv(Ngram4, file = "Ngram4.csv")
Ngram_4T <- data.frame()
numReps <- as.integer(NROW(Ngram4T)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram4T[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_4T <- rbind(Ngram_4T,NgramName)
    z <- z + 1
}
write.csv(Ngram_4T, file = "Ngram_4T.csv")

```
### Create 5-Ngram with the indivual words split and added to additional columns
```{r}

# Create unified 5 NGram dataframe and csv files
Ngram5.1 <-as.data.frame(read.csv("WF_5gram1.csv"))
Ngram5.2 <-as.data.frame(read.csv("WF_5gram2.csv"))
Ngram5.3 <-as.data.frame(read.csv("WF_5gram3.csv"))
Ngram5.4 <-as.data.frame(read.csv("WF_5gram4.csv"))
Ngram5.5 <-as.data.frame(read.csv("WF_5gram5.csv"))
Ngram5.6 <-as.data.frame(read.csv("WF_5gram6.csv"))
Ngram5.7 <-as.data.frame(read.csv("WF_5gram7.csv"))
Ngram5.8 <-as.data.frame(read.csv("WF_5gram8.csv"))

Ngram5 <- rbind(Ngram5.1,Ngram5.2,Ngram5.3,Ngram5.4,Ngram5.5,Ngram5.6,Ngram5.7,Ngram5.8)

Ngram5 <- Ngram5 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram5$word <- gsub('[0-9]+', '', Ngram5$word)
ngramOrder <- order(Ngram5$word, decreasing=FALSE)
Ngram5 <- Ngram5[ngramOrder,]
write.csv(Ngram5, file = "Ngram5.csv")

# Create dataframe column for each word in the string
Ngram_5 <- data.frame()
numReps <- as.integer(NROW(Ngram5)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram5[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_5 <- rbind(Ngram_5,NgramName)
    z <- z + 1
}
write.csv(Ngram_5, file = "Ngram_5.csv")
```

### Create unified 6 NGram dataframe and csv files
```{r}
Ngram6.1 <-as.data.frame(read.csv("WF_6gram1.csv"))
Ngram6.2 <-as.data.frame(read.csv("WF_6gram2.csv"))
Ngram6.3 <-as.data.frame(read.csv("WF_6gram3.csv"))
Ngram6.4 <-as.data.frame(read.csv("WF_6gram4.csv"))
Ngram6.5 <-as.data.frame(read.csv("WF_6gram5.csv"))
Ngram6.6 <-as.data.frame(read.csv("WF_6gram6.csv"))
Ngram6.7 <-as.data.frame(read.csv("WF_6gram7.csv"))
Ngram6.8 <-as.data.frame(read.csv("WF_6gram8.csv"))

Ngram6 <- rbind(Ngram6.1,Ngram6.2,Ngram6.3,Ngram6.4,Ngram6.5,Ngram6.6,Ngram6.7,Ngram6.8)

Ngram6 <- Ngram6 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram6$word <- gsub('[0-9]+', '', Ngram6$word)
ngramOrder <- order(Ngram6$word, decreasing=FALSE)
Ngram6 <- Ngram6[ngramOrder,]
write.csv(Ngram6, file = "Ngram6.csv")

# Create dataframe column for each word in the string
Ngram_6 <- data.frame()
numReps <- as.integer(NROW(Ngram6)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram6[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_6 <- rbind(Ngram_6,NgramName)
    z <- z + 1
}
write.csv(Ngram_6, file = "Ngram_6.csv")
```
```{r}
# Create unified 7 NGram dataframe and csv files
Ngram7.1 <-as.data.frame(read.csv("WF_7gram1.csv"))
Ngram7.2 <-as.data.frame(read.csv("WF_7gram2.csv"))
Ngram7.3 <-as.data.frame(read.csv("WF_7gram3.csv"))
Ngram7.4 <-as.data.frame(read.csv("WF_7gram4.csv"))
Ngram7.5 <-as.data.frame(read.csv("WF_7gram5.csv"))
Ngram7.6 <-as.data.frame(read.csv("WF_7gram6.csv"))
Ngram7.7 <-as.data.frame(read.csv("WF_7gram7.csv"))
Ngram7.8 <-as.data.frame(read.csv("WF_7gram8.csv"))

Ngram7 <- rbind(Ngram7.1,Ngram7.2,Ngram7.3,Ngram7.4,Ngram7.5,Ngram7.6,Ngram7.7,Ngram7.8)

Ngram7 <- Ngram7 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram7$word <- gsub('[0-9]+', '', Ngram7$word)
ngramOrder <- order(Ngram7$word, decreasing=FALSE)
Ngram7 <- Ngram7[ngramOrder,]

write.csv(Ngram7, file = "Ngram7.csv")

# Create dataframe column for each word in the string
Ngram_7 <- data.frame()
numReps <- as.integer(NROW(Ngram7)/10000)

begin <- 1
end <- 10000
z <- 1

while ( z <= numReps) { 
    NgramName <- Ngram7[begin:end,]
    # split 2-gram into 2 additonal columns
    b <- NROW(NgramName)
    a <- 1
    while(a <= b){
        vec <-NgramName[a,1]
        newVec <- (unlist(sapply(vec, strsplit, "\\s+", USE.NAMES = FALSE))) 
        y <- 1
        x <- 3
        #Ngram2[a,x] <- newVec[1]

        # assign members to loop
        while(y <= length(newVec)){
            NgramName[a,x] <- newVec[y]
            y = y + 1
            x <- x + 1
            }
        a <- a + 1
    }
    # increment counters
    temp <- end
    begin <- end + 1
    end <- temp + 10000

   # combine all Ngram2 with expanded columns
    Ngram_7 <- rbind(Ngram_7,NgramName)
    z <- z + 1
}
write.csv(Ngram_7, file = "Ngram_7.csv")
```
### Create unified 8 NGram dataframe and csv files
```{r}
Ngram8.1 <-as.data.frame(read.csv("WF_8gram1.csv"))
Ngram8.2 <-as.data.frame(read.csv("WF_8gram2.csv"))
Ngram8.3 <-as.data.frame(read.csv("WF_8gram3.csv"))
Ngram8.4 <-as.data.frame(read.csv("WF_8gram4.csv"))
Ngram8.5 <-as.data.frame(read.csv("WF_8gram5.csv"))
Ngram8.6 <-as.data.frame(read.csv("WF_8gram6.csv"))
Ngram8.7 <-as.data.frame(read.csv("WF_8gram7.csv"))
Ngram8.8 <-as.data.frame(read.csv("WF_8gram8.csv"))

Ngram8 <- rbind(Ngram8.1,Ngram8.2,Ngram8.3,Ngram8.4,Ngram8.5,Ngram8.6,Ngram8.7,Ngram8.8)

Ngram8 <- Ngram8 %>% group_by(word) %>% summarise( sum=sum(freq))

Ngram8$word <- gsub('[0-9]+', '', Ngram8$word)
ngramOrder <- order(Ngram8$word, decreasing=FALSE)
Ngram8 <- Ngram8[ngramOrder,]


write.csv(Ngram8, file = "Ngram8.csv")
```
```{r}
ngram_7 <-  read.csv("WF_7gram.csv")
ngram_7b <- as.data.frame( read.csv("WF_7gram2.csv"))
ngram7 <- rbind(ngram_7,ngram_7b)
ngram7$word <- gsub('[0-9]+', '', ngram7$word)
ngramOrder <- order(ngram7$word, decreasing=FALSE)
ngram7O <- ngram7[ngramOrder,]
```

### Create testing dataframe for twitter dataset
```{r}
con <- file("en_us.twitter.txt")
twitterInput <- readLines(con, skipNul = TRUE)
close(con)
# remove non-ascii characters
twitterInput<- gsub("[^\x20-\x7E]", "", twitterInput)
# remove all puncutation but single quote, period, question mark and exclaimation point
twitterInput<- gsub("[^'.?![:^punct:]]", "", twitterInput, perl=TRUE)
# remove numbers
twitterInput <-gsub('[0-9]+', '', twitterInput)
# remove quotation 
twitterInput<- gsub("[\"]", "", twitterInput)



twitterDf <- data.frame(twitterInput)
twitterDfR <-as.data.frame( twitterDf[complete.cases(twitterDf),])
testingDf <- as.data.frame(twitterDfR[74661:77259,]) #testing dataFrame


testCorpus = VCorpus(VectorSource(testingDf)) # create news corpus from dataframe 
testCorpus <- tm_map(testCorpus, tolower)# change all characters to lower case
testCorpus <- tm_map(testCorpus, stripWhitespace)# strip white spaces
testCorpus<- tm_map(testCorpus, PlainTextDocument)

### 4grams identification for twitter testing dataset
methods(findAssocs )
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm <- TermDocumentMatrix(testCorpus, control = list(tokenize = BigramTokenizer))
NgramFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
newsTestNgram4 <- as.data.frame(data.table(word=names(NgramFreq), freq=NgramFreq))
outcon <- file(paste("twitterTest.csv", sep=""))
write.csv(twitterTestNgram4, file = outcon)

```


### Developing Markov model for prediction of words with bigram.
```{r}
Ngram_2 <-as.data.frame(read.csv("Ngram_2.csv"))

# input string to predict next word
predictString <- "conduct adequate due diligence"

# Select the first two words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 

# Find the third word
nextWord <- Ngram_2[,4] == newVec[3]

vector_2 <- Ngram_2[nextWord,]
# remove meaning less words
vector_2 <- filter(vector_2, V4 != "a" & V4 != "and" & sum != 1) 
sortOrder <- order(vector_2$sum, decreasing=TRUE)
vectorOrder <- vector_2[sortOrder,]
#vector_4c <- Ngram_4[Ngram_4Secondc,]

fourthWorda <- as.character(vector_4a$V5)
fourthWordb <- as.character(vector_4b$V6)
#fourthWordc <- as.character(vector_4b$V6)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]



```
### Developing Markov model for prediction of words with trigram.
```{r}
Ngram_3 <-as.data.frame(read.csv("Ngram_3.csv"))

# input string to predict next word
predictString <- "you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.


Ngram_3Seconda <- Ngram_3[,4] == newVec[2] &  Ngram_3[,5] == newVec[3]
#Ngram_3Secondb <- Ngram_3[,4] == "to" &  Ngram_3[,5] == "city"


vector_1a <- Ngram_3[Ngram_3Seconda,]
#vector_1b <- Ngram_3[Ngram_3Second,]

```

### Developing Markov model for prediction of words with 4-gram.
```{r}
Ngram_4 <-as.data.frame(read.csv("Ngram_4.csv"))

# input string to predict next word
predictString <- "you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 
pick3 <- paste(newVec[1], newVec[2], newVec[3], sep=" ")

# match last two words of the string and find potential next words.
Ngram_4Seconda <- Ngram_4[,4] == newVec[2] &  Ngram_4[,5] == newVec[3]
Ngram_4Secondb <- Ngram_4[,5] == newVec[2] &  Ngram_4[,6] == newVec[3]
#Ngram_4Secondc <- Ngram_4[,6] == newVec[2] &  Ngram_4[,6] == newVec[3]


vector_4a <- Ngram_4[Ngram_4Seconda,]
vector_4b <- Ngram_4[Ngram_4Secondb,]
#vector_4c <- Ngram_4[Ngram_4Secondc,]

fourthWorda <- as.character(vector_4a$V5)
fourthWordb <- as.character(vector_4b$V6)
#fourthWordc <- as.character(vector_4b$V6)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]



```

### Developing Markov model for prediction of words with 5-gram.
```{r}
Ngram_5 <-as.data.frame(read.csv("Ngram_5.csv"))

# input string to predict next word
predictString <- "attend to city business"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.


# match last two words of the string and find potential next words.

Ngram_5Seconda <- Ngram_5[,3] == newVec[2] &  Ngram_5[,4] == newVec[3]
Ngram_5Secondb <- Ngram_5[,4] == newVec[2] &  Ngram_5[,5] == newVec[3]
Ngram_5Secondc <- Ngram_5[,5] == newVec[2] &  Ngram_5[,6] == newVec[3]
Ngram_5Secondd <- Ngram_5[,6] == newVec[2] &  Ngram_5[,7] == newVec[3]


vector_5a <- Ngram_5[Ngram_5Seconda,]
vector_5b <- Ngram_5[Ngram_5Secondb,]
vector_5c <- Ngram_5[Ngram_5Secondc,]
vector_5d <- Ngram_5[Ngram_5Secondd,]

fourthWorda <- as.character(vector_5a$V4)
fourthWordb <- as.character(vector_5b$V5)
fourthWordc <- as.character(vector_5b$V6)
fourthWordd <- as.character(vector_5b$V7) 

# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb, fourthWordc, fourthWordd)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]

```

### Developing Markov model for prediction of words with 6-gram.
```{r}
Ngram_6 <-as.data.frame(read.csv("Ngram_6.csv"))

# input string to predict next word
predictString <- "then you must be"

# Match the first three words in string
newVec <- (unlist(sapply(predictString, strsplit, "\\s+", USE.NAMES = FALSE))) 


# match last two words of the string and find potential next words.

Ngram_6Seconda <- Ngram_6[,3] == newVec[2] &  Ngram_6[,4] == newVec[3]
Ngram_6Secondb <- Ngram_6[,4] == newVec[2] &  Ngram_6[,5] == newVec[3]
Ngram_6Secondc <- Ngram_6[,5] == newVec[2] &  Ngram_6[,6] == newVec[3]
Ngram_6Secondd <- Ngram_6[,6] == newVec[2] &  Ngram_6[,7] == newVec[3]
Ngram_6Seconde <- Ngram_6[,7] == newVec[2] &  Ngram_6[,8] == newVec[3]


vector_6a <- Ngram_6[Ngram_6Seconda,]
vector_6b <- Ngram_6[Ngram_6Secondb,]
vector_6c <- Ngram_6[Ngram_6Secondc,]
vector_6d <- Ngram_6[Ngram_6Secondd,]
vector_6e <- Ngram_6[Ngram_6Seconde,]
fourthWorda <- as.character(vector_6a$V4)
fourthWordb <- as.character(vector_6b$V5)
fourthWordc <- as.character(vector_6b$V6)
fourthWordd <- as.character(vector_6b$V7)
fourthWorde <- as.character(vector_6b$V8)
# Combine character vectors
fourthWord <- c(fourthWorda, fourthWordb, fourthWordc, fourthWordd, fourthWorde)
# sort vector
fourthWord <- sort(fourthWord)
wordTable <- table(fourthWord)
wordDF <- as.data.frame(wordTable)

# Order word dataframe
wordOrder <- order(wordDF$Freq, decreasing=TRUE)
wordDF <- wordDF[wordOrder,]
```


